{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andres4ramos/167/blob/main/12_1_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook 12.1: Self Attention**\n",
        "\n",
        "This notebook builds a self-attention mechanism from scratch, as discussed in section 12.2 of the book.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
        "\n"
      ],
      "metadata": {
        "id": "t9vk9Elugvmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OLComQyvCIJ7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
        "\n"
      ],
      "metadata": {
        "id": "9OJkkoNqCVK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(3)\n",
        "# Number of inputs\n",
        "N = 3\n",
        "# Number of dimensions of each input\n",
        "D = 4\n",
        "# Create an empty list\n",
        "all_x = []\n",
        "# Create elements x_n and append to list\n",
        "for n in range(N):\n",
        "  all_x.append(np.random.normal(size=(D,1)))\n",
        "# Print out the list\n",
        "print(all_x)\n"
      ],
      "metadata": {
        "id": "oAygJwLiCSri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5953d81a-691c-412d-e791-8f1865199d03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[ 1.78862847],\n",
            "       [ 0.43650985],\n",
            "       [ 0.09649747],\n",
            "       [-1.8634927 ]]), array([[-0.2773882 ],\n",
            "       [-0.35475898],\n",
            "       [-0.08274148],\n",
            "       [-0.62700068]]), array([[-0.04381817],\n",
            "       [-0.47721803],\n",
            "       [-1.31386475],\n",
            "       [ 0.88462238]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)"
      ],
      "metadata": {
        "id": "W2iHFbtKMaDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Choose random values for the parameters\n",
        "omega_q = np.random.normal(size=(D,D))\n",
        "omega_k = np.random.normal(size=(D,D))\n",
        "omega_v = np.random.normal(size=(D,D))\n",
        "beta_q = np.random.normal(size=(D,1))\n",
        "beta_k = np.random.normal(size=(D,1))\n",
        "beta_v = np.random.normal(size=(D,1))"
      ],
      "metadata": {
        "id": "79TSK7oLMobe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute the queries, keys, and values for each input"
      ],
      "metadata": {
        "id": "VxaKQtP3Ng6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_queries = []\n",
        "all_keys = []\n",
        "all_values = []\n",
        "\n",
        "for x in all_x:\n",
        "    query = np.dot(omega_q, x) + beta_q.flatten()\n",
        "    key = np.dot(omega_k, x) + beta_k.flatten()\n",
        "    value = np.dot(omega_v, x) + beta_v.flatten()\n",
        "\n",
        "    all_queries.append(query)\n",
        "    all_keys.append(key)\n",
        "    all_values.append(value)"
      ],
      "metadata": {
        "id": "TwDK2tfdNmw9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a softmax function (equation 12.5) -- here, it will take a list of arbitrary numbers and return a list where the elements are non-negative and sum to one\n"
      ],
      "metadata": {
        "id": "Se7DK6PGPSUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(items_in):\n",
        "    # Shift items_in by its maximum to prevent overflow\n",
        "    exp_items = np.exp(items_in - np.max(items_in))\n",
        "    # Compute softmax output\n",
        "    items_out = exp_items / np.sum(exp_items)\n",
        "\n",
        "    return items_out"
      ],
      "metadata": {
        "id": "u93LIcE5PoiM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now compute the self attention values:"
      ],
      "metadata": {
        "id": "8aJVhbKDW7lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_x_prime = []\n",
        "\n",
        "for n in range(N):\n",
        "    all_km_qn = []\n",
        "    for key in all_keys:\n",
        "        dot_product = np.dot(all_queries[n], key)\n",
        "        all_km_qn.append(dot_product)\n",
        "\n",
        "    attention = softmax(all_km_qn)\n",
        "    print(\"Attentions for output \", n)\n",
        "    print(attention)\n",
        "\n",
        "    x_prime = np.zeros_like(all_values[0])\n",
        "    for i, value in enumerate(all_values):\n",
        "        x_prime += attention[i] * value\n",
        "\n",
        "    all_x_prime.append(x_prime)\n",
        "\n",
        "print(\"x_prime_0_calculated:\", all_x_prime[0].transpose())\n",
        "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
        "print(\"x_prime_1_calculated:\", all_x_prime[1].transpose())\n",
        "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
        "print(\"x_prime_2_calculated:\", all_x_prime[2].transpose())\n",
        "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")\n"
      ],
      "metadata": {
        "id": "yimz-5nCW6vQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ca249d-9f83-44d1-b813-602666f544f5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attentions for output  0\n",
            "[[[2.88201344e-17 1.03158450e-15 2.18850584e-18 1.91057659e-19]\n",
            "  [4.62896540e-07 3.30692046e-10 8.55616425e-05 1.19246540e-02]\n",
            "  [3.40812297e-22 2.26642230e-18 5.99926383e-25 1.48829296e-27]\n",
            "  [2.94734043e-13 1.50215843e-13 4.79001215e-13 7.58286586e-13]]\n",
            "\n",
            " [[2.40927759e-11 8.62373985e-10 1.82952584e-12 1.59718526e-13]\n",
            "  [3.31110125e-15 2.36544184e-18 6.12022853e-13 8.52971093e-11]\n",
            "  [1.76341036e-09 1.17267851e-05 3.10410277e-12 7.70063531e-15]\n",
            "  [7.32126296e-13 3.73139688e-13 1.18985029e-12 1.88360172e-12]]\n",
            "\n",
            " [[3.68249604e-09 1.31810830e-07 2.79636589e-10 2.44124148e-11]\n",
            "  [1.06222536e-18 7.58850948e-22 1.96341383e-16 2.73639331e-14]\n",
            "  [1.48544316e-04 9.87828649e-01 2.61480161e-07 6.48678060e-10]\n",
            "  [6.57684302e-13 3.35199154e-13 1.06886730e-12 1.69207866e-12]]]\n",
            "Attentions for output  1\n",
            "[[[1.18365030e-18 1.64535585e-15 6.43586474e-21 4.63981909e-23]\n",
            "  [1.07504087e-13 7.78321858e-13 2.58208404e-14 6.69926054e-15]\n",
            "  [6.21411778e-16 4.82787939e-14 2.69977494e-17 1.38982326e-18]\n",
            "  [1.44307346e-14 2.63421018e-13 1.78013750e-15 2.45914748e-16]]\n",
            "\n",
            " [[5.65338782e-08 7.85860040e-05 3.07391798e-10 2.21608500e-12]\n",
            "  [7.51802596e-10 5.44299673e-09 1.80571506e-10 4.68495814e-11]\n",
            "  [5.28433005e-09 4.10550766e-07 2.29582095e-10 1.18187088e-11]\n",
            "  [1.60740038e-09 2.93417526e-08 1.98284687e-10 2.73917766e-11]]\n",
            "\n",
            " [[7.18693579e-04 9.99033822e-01 3.90775440e-06 2.81722414e-08]\n",
            "  [1.66660663e-08 1.20661121e-07 4.00293468e-09 1.03856815e-09]\n",
            "  [2.05996794e-06 1.60043262e-04 8.94970133e-08 4.60723705e-09]\n",
            "  [1.08910140e-07 1.98806372e-06 1.34348687e-08 1.85594222e-09]]]\n",
            "Attentions for output  2\n",
            "[[[2.76663883e-08 4.19819693e-08 2.04860903e-08 1.54178954e-08]\n",
            "  [6.90683295e-12 4.78032070e-10 3.26134124e-13 1.81652256e-14]\n",
            "  [4.45527448e-07 1.88011739e-07 8.29555129e-07 1.49352064e-06]\n",
            "  [2.52001497e-10 3.32818269e-09 3.92502890e-11 6.76035940e-12]]\n",
            "\n",
            " [[1.80038902e-06 2.73197484e-06 1.33313144e-06 1.00331887e-06]\n",
            "  [4.15559893e-05 2.87615116e-03 1.96223454e-06 1.09293786e-07]\n",
            "  [6.29016216e-07 2.65443652e-07 1.17120422e-06 2.10862138e-06]\n",
            "  [1.06541565e-05 1.40709399e-04 1.65942951e-06 2.85815472e-07]]\n",
            "\n",
            " [[6.04276522e-06 9.16950857e-06 4.47447757e-06 3.36750577e-06]\n",
            "  [1.40882762e-02 9.75070326e-01 6.65235090e-04 3.70526866e-05]\n",
            "  [4.49839570e-07 1.89831447e-07 8.37584137e-07 1.50797597e-06]\n",
            "  [4.88261284e-04 6.44846469e-03 7.60487408e-05 1.30984213e-05]]]\n",
            "x_prime_0_calculated: [[ 6.08458905e-09  1.53582764e-06  7.06718342e-04  9.47710569e-13]\n",
            " [ 2.49095927e-07  1.17521948e-09  4.93280509e+00  6.85595597e-13]\n",
            " [ 2.64781079e-10  2.23916906e-04  1.06076713e-06 -3.78483527e-13]\n",
            " [ 2.97888441e-11  3.44456192e-02  2.80771398e-09  5.77861488e-13]]\n",
            "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
            "x_prime_1_calculated: [[ 1.18310180e-03 -5.68873372e-09  9.79939610e-06  2.86585261e-07]\n",
            " [ 1.88033864e+00 -1.14309831e-08  7.99195614e-04  5.70739110e-06]\n",
            " [ 3.69395902e-06 -4.29833070e-09  3.62858954e-07  2.57977535e-08]\n",
            " [ 3.42825679e-08 -8.20428569e-10  1.99341459e-08  4.07527431e-09]]\n",
            "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
            "x_prime_2_calculated: [[ 1.15756500e-05 -4.52420950e-03 -7.73792503e-07  1.28473552e-03]\n",
            " [ 2.03834227e-05 -8.23761519e-02 -1.74752434e-07  1.85222437e-02]\n",
            " [ 4.48684649e-06 -6.81227014e-04 -3.42999486e-06  1.45641971e-04]\n",
            " [ 4.56803447e-06 -2.78509298e-05 -4.78750912e-06  2.87198689e-05]]\n",
            "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SUURi-KGoaEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute the same thing, but using matrix calculations.  We'll store the $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ in the columns of a $D\\times N$ matrix, using equations 12.6 and 12.7/8.\n",
        "\n",
        "Note:  The book uses column vectors (for compatibility with the rest of the text), but in the wider literature it is more normal to store the inputs in the rows of a matrix;  in this case, the computation is the same, but all the matrices are transposed and the operations proceed in the reverse order."
      ],
      "metadata": {
        "id": "PJ2vCQ_7C38K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define softmax operation that works independently on each column\n",
        "def softmax_cols(data_in):\n",
        "  # Exponentiate all of the values\n",
        "  exp_values = np.exp(data_in) ;\n",
        "  # Sum over columns\n",
        "  denom = np.sum(exp_values, axis = 0);\n",
        "  # Replicate denominator to N rows\n",
        "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
        "  # Compute softmax\n",
        "  softmax = exp_values / denom\n",
        "  # return the answer\n",
        "  return softmax"
      ],
      "metadata": {
        "id": "obaQBdUAMXXv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "    \"\"\"\n",
        "    Compute self-attention mechanism in matrix form.\n",
        "\n",
        "    Parameters:\n",
        "    - X : Input matrix (NxD) where N is the number of inputs, D is the dimension of each input.\n",
        "    - omega_v, omega_q, omega_k : Weight matrices for values, queries, and keys, respectively.\n",
        "    - beta_v, beta_q, beta_k : Bias vectors for values, queries, and keys, respectively.\n",
        "\n",
        "    Returns:\n",
        "    - X_prime : Output matrix after applying self-attention (NxD).\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Compute queries, keys, and values\n",
        "    queries = X @ omega_q + beta_q   # Shape: (N, D)\n",
        "    keys = X @ omega_k + beta_k      # Shape: (N, D)\n",
        "    values = X @ omega_v + beta_v    # Shape: (N, D)\n",
        "\n",
        "    # Step 2: Compute dot products between queries and keys (Attention Scores)\n",
        "    # Transpose keys to get the correct matrix multiplication shape\n",
        "    attention_scores = queries @ keys.T   # Shape: (N, N)\n",
        "\n",
        "    # Step 3: Apply softmax to calculate attention weights\n",
        "    attention_weights = softmax(attention_scores)  # Shape: (N, N)\n",
        "\n",
        "    # Step 4: Compute weighted sum of values based on attention weights\n",
        "    X_prime = attention_weights @ values  # Shape: (N, D)\n",
        "\n",
        "    return X_prime"
      ],
      "metadata": {
        "id": "gb2WvQ3SiH8r"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy data into matrix\n",
        "X = np.zeros((D, N))\n",
        "X[:, 0] = np.squeeze(all_x[0])\n",
        "X[:, 1] = np.squeeze(all_x[1])\n",
        "X[:, 2] = np.squeeze(all_x[2])\n",
        "\n",
        "# Transpose X to match (N, D) shape\n",
        "X = X.T\n",
        "\n",
        "# Run the self-attention mechanism\n",
        "X_prime = self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "\n",
        "# Print out the results\n",
        "print(X_prime)\n"
      ],
      "metadata": {
        "id": "MUOJbgJskUpl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "3e145c88-50fe-41be-a516-1af00404e14f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (3,4) (4,1) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-7f61ec077563>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Run the self-attention mechanism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Print out the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-0f839f410985>\u001b[0m in \u001b[0;36mself_attention\u001b[0;34m(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Step 1: Compute queries, keys, and values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0momega_q\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_q\u001b[0m   \u001b[0;31m# Shape: (N, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0momega_k\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_k\u001b[0m      \u001b[0;31m# Shape: (N, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0momega_v\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_v\u001b[0m    \u001b[0;31m# Shape: (N, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,4) (4,1) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you did this correctly, the values should be the same as above.\n",
        "\n",
        "TODO:  \n",
        "\n",
        "Print out the attention matrix\n",
        "You will see that the values are quite extreme (one is very close to one and the others are very close to zero.  Now we'll fix this problem by using scaled dot-product attention."
      ],
      "metadata": {
        "id": "as_lRKQFpvz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product self-attention mechanism in matrix form.\n",
        "\n",
        "    Parameters:\n",
        "    - X : Input matrix (NxD) where N is the number of inputs, D is the dimension of each input.\n",
        "    - omega_v, omega_q, omega_k : Weight matrices for values, queries, and keys, respectively.\n",
        "    - beta_v, beta_q, beta_k : Bias vectors for values, queries, and keys, respectively.\n",
        "\n",
        "    Returns:\n",
        "    - X_prime : Output matrix after applying scaled dot-product self-attention (NxD).\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Compute queries, keys, and values\n",
        "    queries = X @ omega_q + beta_q   # Shape: (N, D)\n",
        "    keys = X @ omega_k + beta_k      # Shape: (N, D)\n",
        "    values = X @ omega_v + beta_v    # Shape: (N, D)\n",
        "\n",
        "    # Step 2: Compute dot products between queries and keys (Attention Scores)\n",
        "    attention_scores = queries @ keys.T   # Shape: (N, N)\n",
        "\n",
        "    # Step 3: Scale the dot products\n",
        "    d_k = keys.shape[1]  # Dimension of the key vectors\n",
        "    scaled_attention_scores = attention_scores / np.sqrt(d_k)\n",
        "\n",
        "    # Step 4: Apply softmax to calculate attention weights\n",
        "    attention_weights = softmax(scaled_attention_scores)  # Shape: (N, N)\n",
        "\n",
        "    # Step 5: Compute weighted sum of values based on attention weights\n",
        "    X_prime = attention_weights @ values  # Shape: (N, D)\n",
        "\n",
        "    return X_prime"
      ],
      "metadata": {
        "id": "kLU7PUnnqvIh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_prime = scaled_dot_product_self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "\n",
        "# Print out the results\n",
        "print(X_prime)"
      ],
      "metadata": {
        "id": "n18e3XNzmVgL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "0271e60b-91e3-4714-8c16-b3004f551c32"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (3,4) (4,1) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-6706763e047e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product_self_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Print out the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_prime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-a4b5babc6a3d>\u001b[0m in \u001b[0;36mscaled_dot_product_self_attention\u001b[0;34m(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Step 1: Compute queries, keys, and values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0momega_q\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_q\u001b[0m   \u001b[0;31m# Shape: (N, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0momega_k\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_k\u001b[0m      \u001b[0;31m# Shape: (N, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0momega_v\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_v\u001b[0m    \u001b[0;31m# Shape: (N, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,4) (4,1) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In self-attention, being \"covariant with respect to permutation\" means that if we rearrange the columns of the input matrix, the output matrix should rearrange its columns in the same way. This would happen if the output only depended on the relative positions of elements in the input, rather than their fixed positions. However, self-attention usually relies on learned weights, so it is not naturally \"permutation covariant\" — it’s sensitive to the original order of the sequence. As a result, if we rearrange the input, we will likely get a different output, unless additional mechanisms are added to make the model recognize and handle different orders.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QDEkIrcgrql-"
      }
    }
  ]
}